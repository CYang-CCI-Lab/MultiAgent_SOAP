
class BaselineLLMAgent:
    def __init__(
        self,
        system_prompt: str,
        client=AsyncOpenAI(base_url="http://localhost:8000/v1", api_key="dummy"),
        model_name: str = "meta-llama/Llama-3.3-70B-Instruct"
    ):
        self.client = client
        self.model_name = model_name
        self.messages = [{"role": "system", "content": system_prompt}]

    async def ask(self, user_prompt: str, guided_schema: Optional[Dict[str, Any]] = None) -> Optional[str]:
        self.messages.append({"role": "user", "content": user_prompt})
        params = {
            "model": self.model_name,
            "messages": self.messages,
            "temperature": 0.5
        }


            async def llm_call(
        self, 
        user_prompt: str, 
        temperature: float = 0.5,