{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the skipped instances from MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of items: 400\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# missing_qn = []\n",
    "missing_id = []\n",
    "# Replace 'your_file.json' with the path to your JSON file\n",
    "with open('/home/yl3427/cylab/SOAP_MA/Output/TNM/brca_final_with_baseline_500.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "print(\"Total number of items:\", len(data))\n",
    "\n",
    "for idx, item in enumerate(data):\n",
    "    if len(item.keys()) != 9: # soap, tnm은 9개, medicalqa는 11개\n",
    "        print(\"Error in line\", idx)\n",
    "        print(item['File ID'])\n",
    "        missing_id.append(item['File ID'])\n",
    "#     try:\n",
    "#         question = item['Question']\n",
    "#     except:\n",
    "#         print(\"Error in line\", idx)\n",
    "#         print(item['qn_num'])\n",
    "#         missing_qn.append(item['qn_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# print(missing_qn)\n",
    "print(missing_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, item in enumerate(data):\n",
    "    if item['Fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the skipped instances from Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "missing_item = []\n",
    "# Replace 'your_file.json' with the path to your JSON file\n",
    "with open('/home/yl3427/cylab/SOAP_MA/Output/TNM/brca_only_with_baseline.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for idx, item in enumerate(data):\n",
    "    if item[\"BaselineChoice\"] == \"Unknown\":\n",
    "        print(\"Error in line\", idx)\n",
    "        print(item['filename'])\n",
    "        missing_item.append(item['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T2 T1\n",
      "T2 T1\n",
      "T3 T1\n",
      "T3 T2\n",
      "T2 T1\n",
      "T3 T2\n",
      "T2 T4\n",
      "T1 T2\n",
      "T3 T2\n",
      "T3 T2\n",
      "T1 T2\n",
      "T1 T2\n",
      "T1 T2\n",
      "T2 T4\n",
      "T1 T4\n",
      "T4 T2\n",
      "T3 T2\n",
      "T4 T3\n",
      "T2 T4\n",
      "T3 T2\n",
      "T1 T2\n",
      "T2 T1\n",
      "T2 T1\n",
      "T1 T2\n",
      "T2 T4\n",
      "T3 T2\n",
      "T1 T2\n",
      "T3 T4\n",
      "T3 T2\n",
      "T1 T2\n",
      "T4 T2\n",
      "T3 T1\n",
      "T2 T4\n",
      "T1 T2\n",
      "T1 T2\n",
      "T2 T3\n",
      "T3 T4\n",
      "T1 T2\n",
      "T1 T2\n",
      "T1 T2\n",
      "T3 T2\n",
      "T2 T1\n",
      "T3 T4\n",
      "T3 T4\n",
      "T3 T2\n",
      "T3 T1\n",
      "T1 T2\n",
      "T3 T2\n",
      "T4 T2\n",
      "T4 T2\n",
      "T3 T2\n",
      "T1 T2\n",
      "T3 T1\n",
      "T3 T1\n",
      "T1 T2\n",
      "T2 T1\n",
      "T3 T1\n",
      "T1 T2\n",
      "T3 T1\n",
      "T3 T2\n",
      "T1 T2\n",
      "T3 T4\n",
      "T1 T2\n",
      "T1 T2\n",
      "T1 T2\n",
      "T2 T1\n",
      "T1 T2\n",
      "T1 T2\n",
      "T3 T2\n",
      "T3 T2\n",
      "T1 T2\n",
      "T2 T3\n",
      "T1 T2\n",
      "T3 T2\n",
      "T1 T2\n",
      "T2 T4\n",
      "T4 T2\n",
      "T4 T1\n",
      "T2 T3\n",
      "T3 T4\n",
      "T1 T3\n",
      "T1 T2\n",
      "T3 T4\n",
      "T1 T2\n",
      "T1 T2\n",
      "T2 T4\n",
      "T3 T2\n",
      "T2 T4\n",
      "T1 T2\n",
      "T1 T2\n",
      "T2 T4\n",
      "T4 T3\n",
      "T2 T1\n",
      "T2 T4\n",
      "T2 T1\n",
      "T2 T1\n",
      "T2 T4\n",
      "T2 T4\n",
      "T1 T2\n",
      "T3 T4\n",
      "T2 T3\n",
      "T1 T2\n",
      "T1 T2\n",
      "T1 T2\n",
      "T2 T1\n",
      "T2 T1\n",
      "T1 T2\n",
      "T1 T4\n",
      "T4 T2\n",
      "T4 T2\n",
      "T3 T2\n",
      "T4 T2\n",
      "T2 T1\n",
      "T3 T2\n",
      "114\n"
     ]
    }
   ],
   "source": [
    "error_cnt = 0\n",
    "for idx, item in enumerate(data):\n",
    "    label = f\"T{item['Answer']+1}\"\n",
    "    baseline_pred = item['BaselineChoice']\n",
    "    if label != baseline_pred:\n",
    "        print(label, baseline_pred)\n",
    "        error_cnt += 1\n",
    "print(error_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_qn = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample_data_with_baseline.json'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_json_path = \"sample_data.json\"\n",
    "f\"{input_json_path.split('.')[0]}_with_baseline.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/yl3427/cylab/llm_reasoning/reasoning/data/step3_ALL.csv\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_data', 'json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_json_path.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 1 items from file1 and 351 items from file2.\n",
      "Final list contains 351 items.\n",
      "Merged file saved as: /home/yl3427/cylab/SOAP_MA/Output/SOAP/chf_final.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import copy\n",
    "import os\n",
    "\n",
    "def deep_merge(dict1, dict2):\n",
    "    \"\"\"\n",
    "    Recursively merge dict2 into dict1.\n",
    "    - If a key exists in both dicts and both values are themselves dicts, \n",
    "      recurse into those.\n",
    "    - Otherwise, dict2’s value overwrites dict1’s value.\n",
    "    \"\"\"\n",
    "    for key, value in dict2.items():\n",
    "        if key not in dict1:\n",
    "            dict1[key] = value\n",
    "        else:\n",
    "            if isinstance(dict1[key], dict) and isinstance(value, dict):\n",
    "                deep_merge(dict1[key], value)\n",
    "            else:\n",
    "                # Overwrite dict1's value with dict2's value\n",
    "                dict1[key] = value\n",
    "    return dict1\n",
    "\n",
    "def merge_json_files(file1, file2, output_file, identifier='qn_num'):\n",
    "    \"\"\"\n",
    "    Merges two JSON files (each containing a list of dicts) \n",
    "    on a specified 'identifier' key. \n",
    "    If the key is missing from a particular dict, it will be skipped.\n",
    "    \"\"\"\n",
    "    # 1) Load both JSON files\n",
    "    with open(file1, \"r\", encoding=\"utf-8\") as f:\n",
    "        data1 = json.load(f)  # List of dicts\n",
    "    with open(file2, \"r\", encoding=\"utf-8\") as f:\n",
    "        data2 = json.load(f)  # List of dicts\n",
    "\n",
    "    # 2) Convert the first list into a dictionary keyed by 'identifier'\n",
    "    merged_dict = {}\n",
    "    for item in data1:\n",
    "        if identifier in item:\n",
    "            key_value = item[identifier]\n",
    "            merged_dict[key_value] = copy.deepcopy(item)\n",
    "\n",
    "    # 3) For each item in data2, either add or merge\n",
    "    for item in data2:\n",
    "        if identifier not in item:\n",
    "            # Skip if 'identifier' doesn't exist in this item\n",
    "            continue\n",
    "        key_value = item[identifier]\n",
    "        if key_value not in merged_dict:\n",
    "            merged_dict[key_value] = copy.deepcopy(item)\n",
    "        else:\n",
    "            # Deep-merge fields from item into existing\n",
    "            deep_merge(merged_dict[key_value], item)\n",
    "\n",
    "    # 4) Convert the merged dictionary back to a list\n",
    "    final_list = list(merged_dict.values())\n",
    "\n",
    "    print(f\"Merged {len(data1)} items from file1 and {len(data2)} items from file2.\")\n",
    "    print(f\"Final list contains {len(final_list)} items.\")\n",
    "\n",
    "    # 5) Write to the output file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_list, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    file1 = \"/home/yl3427/cylab/SOAP_MA/Output/SOAP/chf_missing_final.json\"\n",
    "    file2 = \"/home/yl3427/cylab/SOAP_MA/Output/SOAP/chf_final.json\"\n",
    "    output_file = \"/home/yl3427/cylab/SOAP_MA/Output/SOAP/chf_final.json\"\n",
    "\n",
    "    # Example identifier: 'File ID' or 'qn_num'\n",
    "    # If you need to merge on 'File ID', just pass identifier='File ID'.\n",
    "    merge_json_files(file1, file2, output_file, identifier='File ID')\n",
    "    print(f\"Merged file saved as: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# df = pd.read_csv(\"/home/yl3427/cylab/SOAP_MA/Input/SOAP_5_problems.csv\")\n",
    "df = pd.read_csv(\"/home/yl3427/cylab/SOAP_MA/Input/step3_ALL.csv\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_path = \"/home/yl3427/cylab/SOAP_MA/Output/SOAP/sepsis_final.json\"\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 80\n",
      "True negatives: 131\n",
      "False positives: 93\n",
      "False negatives: 24\n",
      "Other: 23\n",
      "Precision: 0.4624277456647399\n",
      "Recall: 0.7692307692307693\n",
      "F1 Score: 0.5776173285198556\n"
     ]
    }
   ],
   "source": [
    "with open('/home/yl3427/cylab/SOAP_MA/Output/SOAP/sepsis_final_with_baseline.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "_ = 0\n",
    "\n",
    "for item_dict in data:\n",
    "    pred = item_dict['BaselineChoice']\n",
    "    if 'sepsis' in item_dict['Answer']:\n",
    "        label = \"Yes\"\n",
    "    elif 'septic' in item_dict['Answer']:\n",
    "        label = \"Not sure\"\n",
    "    else:\n",
    "        label = \"No\"\n",
    "\n",
    "    if pred == \"Yes\" and label == \"Yes\":\n",
    "        tp += 1\n",
    "    elif pred == \"Yes\" and label == \"No\":\n",
    "        fp += 1\n",
    "    elif pred == \"No\" and label == \"No\":\n",
    "        tn += 1\n",
    "    elif pred == \"No\" and label == \"Yes\":\n",
    "        fn += 1\n",
    "    else:\n",
    "        _ += 1\n",
    "\n",
    "print(\"True positives:\", tp)\n",
    "print(\"True negatives:\", tn)\n",
    "print(\"False positives:\", fp)\n",
    "print(\"False negatives:\", fn)\n",
    "print(\"Other:\", _)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def evaluate_SOAP_predictions(\n",
    "    file_path,\n",
    "    terms_lst: list,\n",
    "    aggregator_prediction_key=\"final_choice\",\n",
    "    baseline_prediction_key=\"BaselineChoice\",\n",
    "):\n",
    "   \n",
    "    def get_label(answer, terms_lst):\n",
    "        for term in terms_lst:\n",
    "            if term.lower() in answer.lower():\n",
    "                return \"Yes\"\n",
    "        return \"No\"\n",
    "\n",
    "    def compute_confusion_matrix(data_list, prediction_key, is_aggregator=True):\n",
    "      \n",
    "        tp = fp = tn = fn = other = 0\n",
    "\n",
    "        for item in data_list:\n",
    "            if is_aggregator:\n",
    "                # aggregator prediction is nested: item[aggregator_key][aggregator_prediction_key]\n",
    "                pred = item[\"Aggregator\"][prediction_key]\n",
    "            else:\n",
    "                # baseline prediction is top-level: item[baseline_prediction_key]\n",
    "                pred = item[prediction_key]\n",
    "\n",
    "            label = get_label(item[\"Answer\"], terms_lst)\n",
    "\n",
    "            if pred == \"Yes\" and label == \"Yes\":\n",
    "                tp += 1\n",
    "            elif pred == \"Yes\" and label == \"No\":\n",
    "                fp += 1\n",
    "            elif pred == \"No\" and label == \"No\":\n",
    "                tn += 1\n",
    "            elif pred == \"No\" and label == \"Yes\":\n",
    "                fn += 1\n",
    "            else:\n",
    "                # e.g., pred is \"Yes\" but label is \"Not sure\", or vice versa\n",
    "                other += 1\n",
    "\n",
    "        return tp, tn, fp, fn, other\n",
    "\n",
    "    def calculate_metrics(tp, tn, fp, fn):\n",
    "        precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0.0\n",
    "\n",
    "        return {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1\n",
    "        }\n",
    "\n",
    "    def print_results(title, tp, tn, fp, fn, other, metrics):\n",
    "        \"\"\"\n",
    "        Nicely prints out the confusion matrix and performance metrics.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{title}\")\n",
    "        print(\"-\" * len(title))\n",
    "        print(f\"{'True Positives (TP)':25} : {tp}\")\n",
    "        print(f\"{'True Negatives (TN)':25} : {tn}\")\n",
    "        print(f\"{'False Positives (FP)':25} : {fp}\")\n",
    "        print(f\"{'False Negatives (FN)':25} : {fn}\")\n",
    "        print(f\"{'Other':25} : {other}\")\n",
    "\n",
    "        print()\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"{metric_name:25} : {value:.3f}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. Load data from JSON file\n",
    "    # ----------------------------\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 3. Compute confusion matrix & metrics for aggregator\n",
    "    # ---------------------------------------------------\n",
    "    agg_tp, agg_tn, agg_fp, agg_fn, agg_other = compute_confusion_matrix(\n",
    "        data, aggregator_prediction_key, is_aggregator=True\n",
    "    )\n",
    "    agg_metrics = calculate_metrics(agg_tp, agg_tn, agg_fp, agg_fn)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 4. Compute confusion matrix & metrics for baseline\n",
    "    # -------------------------------------------------\n",
    "    base_tp, base_tn, base_fp, base_fn, base_other = compute_confusion_matrix(\n",
    "        data, baseline_prediction_key, is_aggregator=False\n",
    "    )\n",
    "    base_metrics = calculate_metrics(base_tp, base_tn, base_fp, base_fn)\n",
    "\n",
    "    print_results(\"Aggregator Agent Performance\", agg_tp, agg_tn, agg_fp, agg_fn, agg_other, agg_metrics)\n",
    "    print_results(\"Baseline Performance\", base_tp, base_tn, base_fp, base_fn, base_other, base_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregator Agent Performance\n",
      "----------------------------\n",
      "True Positives (TP)       : 39\n",
      "True Negatives (TN)       : 217\n",
      "False Positives (FP)      : 42\n",
      "False Negatives (FN)      : 53\n",
      "Other                     : 0\n",
      "\n",
      "Precision                 : 0.481\n",
      "Recall                    : 0.424\n",
      "F1 Score                  : 0.451\n",
      "\n",
      "Baseline Performance\n",
      "--------------------\n",
      "True Positives (TP)       : 37\n",
      "True Negatives (TN)       : 209\n",
      "False Positives (FP)      : 50\n",
      "False Negatives (FN)      : 55\n",
      "Other                     : 0\n",
      "\n",
      "Precision                 : 0.425\n",
      "Recall                    : 0.402\n",
      "F1 Score                  : 0.413\n"
     ]
    }
   ],
   "source": [
    "mi = [\"myocardial infarction\", \"elevation mi\", \"non-stemi\", \" NSTEMI\", \" stemi\"]\n",
    "chf = [\"congestive heart failure\", \" chf\", \"HFrEF\", \"HFpEF\"]\n",
    "pulmonary_embolism = [\"pulmonary embolism\"]\n",
    "pulmonary_hypertension = [\"pulmonary hypertension\", \"pulmonary htn\"]\n",
    "sepsis = [\"sepsis\", \"septic shock\"]\n",
    "urosepsis = [\"urosepsis\"]\n",
    "meningitis = [\"meningitis\"]\n",
    "aki = [\"acute kidney injury\", \" aki\", \"acute renal failure\", \" arf\"] # -> Acute tubular necrosis (ATN)인가 아닌가\n",
    "atn = [\"acute tubular necrosis\", \" atn\"]\n",
    "pancreatitis = [\"pancreatitis\"]\n",
    "gi_bleed = [\"gastrointestinal bleed\", \"gi bleed\"]\n",
    "hepatitis = [\"hepatitis\", \" hep\"]\n",
    "cholangitis = [\"cholangitis\"]\n",
    "asp_pneumonia = [\"aspiration pneumonia\"]\n",
    "evaluate_SOAP_predictions(\"/home/yl3427/cylab/SOAP_MA/Output/SOAP/chf_final_with_baseline.json\", chf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medical QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시발 존나 짜증나네. 오지선다 아닌것도 있네 시발. 아오 좆같아시발.\n",
    "with open(\"/home/yl3427/cylab/SOAP_MA/Output/MedicalQA/step1_final_with_baseline.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "# for k, v in data[0].items():\n",
    "    # print(k, v)\n",
    "\n",
    "import re\n",
    "\n",
    "for item in data:\n",
    "    text = item[\"Question\"].split(\"\\n\")[-1]\n",
    "    matches = re.findall(r'\\b[A-Z](?=[.:])', text)\n",
    "    print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# If you have scikit-learn installed:\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "def evaluate_multiclass_predictions(\n",
    "    data,\n",
    "    aggregator_key=\"Aggregator\",\n",
    "    aggregator_prediction_key=\"final_choice\",\n",
    "    baseline_prediction_key=\"BaselineChoice\",\n",
    "    classes=[\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
    "):\n",
    "    \n",
    "    # ------------------------------------\n",
    "    # 2. Collect gold labels and predictions\n",
    "    # ------------------------------------\n",
    "    gold_labels_agg = []\n",
    "    pred_labels_agg = []\n",
    "\n",
    "    gold_labels_base = []\n",
    "    pred_labels_base = []\n",
    "\n",
    "    for item in data:\n",
    "        # The gold label is presumably in item[\"Answer\"], e.g. 'A', 'B', etc.\n",
    "        gold = item[\"Answer\"]\n",
    "        \n",
    "        # Aggregator's prediction\n",
    "        # e.g. item[\"Aggregator\"][\"final_choice\"] is 'A', 'B', 'C', etc.\n",
    "        agg_pred = item[aggregator_key][aggregator_prediction_key]\n",
    "\n",
    "        # Baseline's prediction\n",
    "        base_pred = item[baseline_prediction_key]\n",
    "\n",
    "        gold_labels_agg.append(gold)\n",
    "        pred_labels_agg.append(agg_pred)\n",
    "\n",
    "        gold_labels_base.append(gold)\n",
    "        pred_labels_base.append(base_pred)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 3. Build confusion matrices & compute stats\n",
    "    #    (Using scikit-learn for convenience)\n",
    "    # ------------------------------------------------\n",
    "\n",
    "    print(\"== Aggregator Results ==\")\n",
    "    cm_agg = confusion_matrix(gold_labels_agg, pred_labels_agg, labels=classes)\n",
    "    print(\"Confusion Matrix (rows=True label, cols=Predicted):\")\n",
    "    print(cm_agg, \"\\n\")\n",
    "\n",
    "    accuracy_agg = accuracy_score(gold_labels_agg, pred_labels_agg)\n",
    "    print(f\"Accuracy: {accuracy_agg:.3f}\\n\")\n",
    "\n",
    "    # classification_report prints precision, recall, f1 for each class,\n",
    "    # plus macro/micro/weighted averages\n",
    "    report_agg = classification_report(\n",
    "        gold_labels_agg, \n",
    "        pred_labels_agg,\n",
    "        labels=classes,\n",
    "        zero_division=0  # to handle any class with no predictions\n",
    "    )\n",
    "    print(report_agg)\n",
    "\n",
    "    print(\"\\n== Baseline Results ==\")\n",
    "    cm_base = confusion_matrix(gold_labels_base, pred_labels_base, labels=classes)\n",
    "    print(\"Confusion Matrix (rows=True label, cols=Predicted):\")\n",
    "    print(cm_base, \"\\n\")\n",
    "\n",
    "    accuracy_base = accuracy_score(gold_labels_base, pred_labels_base)\n",
    "    print(f\"Accuracy: {accuracy_base:.3f}\\n\")\n",
    "\n",
    "    report_base = classification_report(\n",
    "        gold_labels_base, \n",
    "        pred_labels_base,\n",
    "        labels=classes,\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(report_base)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Aggregator Results ==\n",
      "Confusion Matrix (rows=True label, cols=Predicted):\n",
      "[[63  0  0  0  3]\n",
      " [ 0 48  3  0  0]\n",
      " [ 6  0 36  0  0]\n",
      " [ 0  0  6 63  0]\n",
      " [ 0  3  3  0 33]] \n",
      "\n",
      "Accuracy: 0.910\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.91      0.95      0.93        66\n",
      "           B       0.94      0.94      0.94        51\n",
      "           C       0.75      0.86      0.80        42\n",
      "           D       1.00      0.91      0.95        69\n",
      "           E       0.92      0.85      0.88        39\n",
      "\n",
      "    accuracy                           0.91       267\n",
      "   macro avg       0.90      0.90      0.90       267\n",
      "weighted avg       0.92      0.91      0.91       267\n",
      "\n",
      "\n",
      "== Baseline Results ==\n",
      "Confusion Matrix (rows=True label, cols=Predicted):\n",
      "[[60  0  3  0  3]\n",
      " [ 0 48  3  0  0]\n",
      " [ 3  3 36  0  0]\n",
      " [ 0  3  6 57  3]\n",
      " [ 0  3  3  0 33]] \n",
      "\n",
      "Accuracy: 0.876\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.95      0.91      0.93        66\n",
      "           B       0.84      0.94      0.89        51\n",
      "           C       0.71      0.86      0.77        42\n",
      "           D       1.00      0.83      0.90        69\n",
      "           E       0.85      0.85      0.85        39\n",
      "\n",
      "    accuracy                           0.88       267\n",
      "   macro avg       0.87      0.88      0.87       267\n",
      "weighted avg       0.89      0.88      0.88       267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "filtered_data = []\n",
    "total_data_len = 0\n",
    "for i in range(1, 4):\n",
    "    with open(\"/home/yl3427/cylab/SOAP_MA/Output/MedicalQA/step1_final_with_baseline.json\", 'r') as file:\n",
    "        data = json.load(file)\n",
    "    total_data_len += len(data)\n",
    "\n",
    "    for item in data:\n",
    "        text = item[\"Question\"].split(\"\\n\")[-1]\n",
    "        matches = re.findall(r'\\b[A-Z](?=[.:])', text)\n",
    "        if set(matches) == {'A', 'B', 'C', 'D', 'E'}:\n",
    "            filtered_data.append(item)\n",
    "\n",
    "evaluate_multiclass_predictions(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267, 357)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_data), total_data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
